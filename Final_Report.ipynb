{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42baa36e-745d-4900-b822-f455b3eb2393",
   "metadata": {},
   "source": [
    "# Final Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d24459e-4687-4d04-8e1a-91543ed3bda7",
   "metadata": {
    "tags": []
   },
   "source": [
    "* Name: Tatev Kyosababyan\n",
    "* Subject: DS5001 Exploratory Text Analytics\n",
    "* Professor: Rafael Alvarado\n",
    "* Date: May 2023\n",
    "* <a href=\"https://github.com/tatevkyosababyan/DS-5001-final-project\">GitHub Repository</a>\n",
    "* <a href=\"https://www.dropbox.com/scl/fo/mow1a04rvnnyu5i4a1t4k/h?dl=0&rlkey=q5ieqesghz7n75263zw7bgit4\">Box Link</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c29b6b9-e69c-491b-bc2b-8507cfb06624",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46240ff2-60a2-49a9-b73f-8facab0b5bc0",
   "metadata": {},
   "source": [
    "Digital data has impacted life in many ways, as the field of text analytis has been getting more advanced. With textual data being generated on a daily basis through numerours platforms, the opportunities for further research and experiments have increased significantly. As a result, it is possible to gain insights through diligent analysis of unstructured data. The goal of this project is to analyze scraped data from news articles published by two major news networks, CNN and CNBC, within the time frame of September through November of 2020.  </br>\n",
    "The original dataset comes from 10TB of clickstream data obtained from a company, which represent websites that users have visited. For the purposes of this project, a subset was obtained including the clicks exclusively for CNN and CNBC articles, and the websites were then scraped. The project involves obtaining a LIB table with the scraped data, which then is expanded into a CORPUS table with the cleaned and tokenized text data, and a VOCAB table with unique words from the text data. </br>\n",
    "The main focus of the project is to analyze the text data using various techniques such as TFIDF, PCA, Topic Modeling, and word2vec. With the help of TFIDF tables, we analyze the most important words in the text data on different levels. PCA (Principal Component Analysis) helps reduce the dimensionality of the data for visualization putposes. Topic modeling is used to identify the underlying topics included in the text data, and, finally, word2vec helps generate word embeddings for semantic analysis. </br>\n",
    "The long-term goal of this project is analyzing the spread of misinformation through the resources on the internet. However, in this paper, our focus will be only on general analysis of the data. </br>\n",
    "To sum up, the goal of this project is to provide insights into the topics and patterns found in news articles from CNN and CNBC from the time frame around the 2020 elections in the United States by using text analytics techniques. The results of this and other similar projects can be useful for trend analysis and predictive modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b6c5ea-af94-4e8e-9432-627ab7be8cfd",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b20d75-1e35-4812-b172-b007679f347e",
   "metadata": {},
   "source": [
    "The data were obtained from an organization that has collected clicks and searches conducted by individuals starting from 2012. While the compressed version of the original data are over 10TB large, the focus of this particular analysis is only on the portion from the time frame of September through November of the year 2020. The data were further filtered into obtaining the URL's specifically for two of the major news networks, CNBC and CNN. Moreover, samples of about 500 were obtained from each of these domains, providing around one thousand news articles for this analysis. The articles were then scraped and stored in a dataset.  </br>\n",
    "A sample of the original data, the filtering process and the code used for scraping all articles from that sample can be found in this <a href=\"https://github.com/tatevkyosababyan/DS-5001-final-project/tree/main/files\">folder</a>. The data are scraped using the Python library BeautifulSoup. The titles of the articles, and the dates they were published are obtained from the URL's. As a result, we have the following tables: </br>\n",
    "* The LIB table with the information about the source, the URL, the date published, and the scraped data. </br> \n",
    "* The CORPUS table, where the indexing is done with the variables broken down into the OHCO structure by the source, article, sentences, tokens. Here, there is a column for the tokens, as well as the parts of speech they represent.  </br>\n",
    "* The VOCAB table includes term_str as the index and other features: the frequency ('n') of the given term, Max POS, POS ambiguity, presence of any stopwords, snowball, and lancaster stems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6b5cc4-dc75-44ce-aa95-8ec8a6be8ce6",
   "metadata": {},
   "source": [
    "## Models And Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da3753b-13ab-4bad-bead-710ebfd10dd5",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1c4573-18c0-4a57-a1ba-8b4dbb39bad6",
   "metadata": {},
   "source": [
    "PCA, Principal Component Analysis, is a technique used for dimensionality reducation that aims to obtain reduced dimensions without losing existing information. The main goal is to identify any possible relationships and patterns about the entire data with the help of the reduced data. It finds linear combinations of the original features explaining the most variance in the data, and, as a result, the principal components are ordered in terms of their importance. More specifically, the first principal component explains the most variance in the data. </br>\n",
    "The image below visualizes the principal components of the highest importance obtained through the PCA analysis. along with their explained variance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0129976c-8e69-4301-b428-326c6ef83ef3",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"./PCA4.png\" alt=\"Figure\" width=\"500\" height=\"500\"> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4514bd8-4152-40d4-a478-c53f5c992ed2",
   "metadata": {},
   "source": [
    "In the heatmap below, we can see the terms with the strongest positive and negative weights in each of the 10 principal components. In Principal Component Analysis, the loadings matrix indicates the weights of each term in the principal components, showing how much a given term contributes to the variation captured by the components. For example, from the Figure below, we could say that the word \"election\" is strongly assiciated with the variation captured by PC0 since it has the highest value of all. This would also mean that the articles containing the term \"election\" would likely have high scores on PC0. On the other hand, we can notice that the same word does not have a very strong association in the second principal component based on our visualization. Overall, we can see the underlying structure of the data and we can identify the levels of association for each term with every principal component within the range of our 10 componens for this visualization specifically. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fc9483-ccb6-476b-b46a-d8bb82679fcb",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"./PCA5.png\" alt=\"Figure\" width=\"700\" height=\"700\"> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bfc80f-d360-4207-b593-3710aff1817c",
   "metadata": {},
   "source": [
    "Below we can see another summary of the results from our PCA analysis. The given dataframe represents the top 10 principal components with the ones on the left side being the highest positive, and the ones on the right side being the highest negative weights respectively, indicating the type of association with the variation captured by the corresponding principal component. </br>\n",
    "Looking at the PC0 data as an example, we can see it is highly associated with election-related topics, while the association with stock and market-related topics is negative. Similar analysis can be done for the other rows to develop a better interpret the formation of the principal components with our data.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7761c032-9c59-4bf2-8e69-3a53f10b2ee6",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"./PCA6.png\" alt=\"Figure\" width=\"700\" height=\"700\"> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d476dd3-d7f0-44c6-b1f6-f58c29d9e51a",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e3da3b-585e-416d-aaa3-797d3faedd8b",
   "metadata": {},
   "source": [
    "Word embeddings are dense vector representations of words capturing their semantic meaning and context. Here we use Word2Vec, which is a neural network-based model that takes large data containing text as input and maps words to high-dimensional vectors so that words with similar meanings appear in similar vectors. </br>\n",
    "For this analysis, the focus is on two different word embeddings, aiming to compare the two based on their results. The two scatter plots represent the two of our sources and their articles in them, with the one on the left representing the CNN data, and the one on the right being a representation of our CNBC data. The coordinates of x and y for every word have been obtained by applying t-SNE dimensionality reduction technique to the high-dimensional word vectors. The words are also colored based on what part of speech they represent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60db21a5-b6de-46d6-827e-6f36c32fdf62",
   "metadata": {},
   "source": [
    "<div style=\"overflow: hidden;\">\n",
    "  <img src=\"./CNN_gensim.png\" alt=\"CNN_gensim\" style=\"float: left; width: 50%;\">\n",
    "  <img src=\"./CNBC_gensim.png\" alt=\"CNBC_gensim\" style=\"float: right; width: 50%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37a6669-1705-4c66-a7b8-613c9280149a",
   "metadata": {},
   "source": [
    "For further analysis, I tried comparing what some keywords are associated with for CNN and CNBC. The dataframes below represent the similar terms of the word \"Trump\" from the CNN and the CNBC data respectively. We can see that overall the results seem very similar among the two datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274b70ed-37dd-42a6-b3b7-78967d43df2d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./gensimCNN.png\" alt=\"CNN_gensim\" style=\"display: inline-block; width: 18%;\">\n",
    "  <img src=\"./gensimCNBC.png\" alt=\"CNBC_gensim\" style=\"display: inline-block; width: 18%;\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7699436-35c5-447c-933f-ea97f2ace9ad",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f587d4-b014-4293-a7e7-c7c1d9a8096c",
   "metadata": {},
   "source": [
    "Sentiment analysis is the process of using machine learning and natural language processing techniques to identify subjective information give textual data. Subjectivity represents the degree to which a statement expresses personal feelings or beliefs as opposed to expressing facts. On the other hand, polarity scores quantify the sentimant expressed in text, giving positive, negative, or neutral orientation of the sentiment. </br>\n",
    "As a part of this analysis, the subjectivity and polarity scores were at first obtained for the titles of each article, and then for the article text itself, to gain an understanding of how much the article titles are correlated with the article texts themselves from the perspective of the subjectivity and polarity. We can see the values obtained in the correlation matrix below. An interesting insight is that the correlation between the subjectivity and polarity scores between the titles and the texts indicate similar results. This kind of information can be useful conducting research especially working with large amount of data to better understand how to interpret subsets, take representative samples, and efficiently conduct analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c1a499-4bd8-4fd9-a5e5-63caed1942f9",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"./sentcorr.png\" alt=\"Figure\" width=\"400\" height=\"400\"> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ac898f-751f-4b6f-9f49-0eca5e902b2b",
   "metadata": {},
   "source": [
    "For further sentiment analysis, the data were broken down into two LIB datasets representing our two resources, CNN and CNBC. The data for each of these datasets were then filtered into those containing \"Trump\" and \"Biden\" separately. As a result of a similar sentiment analysis experiment, the sentiment values were rather close to each other for both of these news websites and the candidates as shown below in the table. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15e2b99-727c-4c34-b77b-70c5cd900b40",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"./tea.png\" alt=\"Figure\" width=\"400\" height=\"400\"> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a77df62-69cf-4cb2-9334-401729c3c021",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7515b000-ade1-49cf-98cf-b1d7655e69a6",
   "metadata": {},
   "source": [
    "LDA is a probabilistic topic modeling technique which helps identify the underlying topics in a corpus of text. The assumtion it makes is that every document in the corpus is a mixture of several topics, and each of the latter is a probability distribution over words. The goal for this model is to best explain the corpus of text. </br>\n",
    "The visualization below is a representation of how different articles belong to certain models. In this particular experiment, 20 topics were generated, and with the help of this heatmap we can see the levels of association to each of this topics for different articles. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b4c56c-b298-4e10-82e5-b1aa6dda7983",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"./Topic.png\" alt=\"Figure\" width=\"700\" height=\"700\"> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe506a8-17dc-45ee-9af4-861df795dc2e",
   "metadata": {},
   "source": [
    "The below table is a sample of 8 topics and associated words. This also helps us find association among different words, such as revenue and cents share."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac68f44-4c3b-4285-8d09-be57b392d8ac",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"./LDA.png\" alt=\"Figure\" width=\"700\" height=\"700\"> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e57cdc-6a13-4643-9a44-0a278381a717",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f7a17b-f5f2-43e2-9249-f07a45c677c0",
   "metadata": {},
   "source": [
    "Overall, the data analysis indicates the strong association of topics with the elections, politics, and individuals having direct involvement with these topics, as expected. With the help of sentiment analysis we were able to find out the potential absence of biases between the two news websites. We were also able to epxlore the associations between different words. As a result of this work we analyzed Topic Modeling using LDA techniques, apply PCA for dimenstionality reduction. </br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
